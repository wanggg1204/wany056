{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f8bb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/07 00:33:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9932239d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "file = 'HeartDataOL.csv'\n",
    "df = spark.read.csv(file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69531f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'age': 1, 'height': 0, 'weight': 1, 'ap_hi': 4, 'ap_lo': 4, 'cholesterol': 4, 'glucose': 4, 'smoke': 7, 'alcohol': 8, 'active': 3, 'cardio': 3, 'gender': 0}\n"
     ]
    }
   ],
   "source": [
    "selected_columns = ['id', 'age', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'glucose', 'smoke', 'alcohol', 'active', 'cardio', 'gender']\n",
    "data_selected = df.select(selected_columns)\n",
    "missing_values = {col: data_selected.filter(data_selected[col].isNull()).count() for col in selected_columns}\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28216bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "id has 0 outliers.\n",
      "id has 0 extreme values.\n",
      "age has 15 outliers.\n",
      "age has 11 extreme values.\n",
      "height has 528 outliers.\n",
      "height has 102 extreme values.\n",
      "weight has 1825 outliers.\n",
      "weight has 176 extreme values.\n",
      "ap_hi has 1437 outliers.\n",
      "ap_hi has 291 extreme values.\n",
      "ap_lo has 4635 outliers.\n",
      "ap_lo has 1139 extreme values.\n",
      "cholesterol has 17615 outliers.\n",
      "cholesterol has 17615 extreme values.\n",
      "glucose has 10521 outliers.\n",
      "glucose has 10521 extreme values.\n",
      "smoke has 6169 outliers.\n",
      "smoke has 6169 extreme values.\n",
      "alcohol has 3764 outliers.\n",
      "alcohol has 3764 extreme values.\n",
      "active has 13736 outliers.\n",
      "active has 13736 extreme values.\n",
      "cardio has 0 outliers.\n",
      "cardio has 0 extreme values.\n",
      "gender has 0 outliers.\n",
      "gender has 0 extreme values.\n",
      "\n",
      "---\n",
      "\n",
      "+---+---+------+------+-----+-----+-----------+-------+-----+-------+------+------+------+\n",
      "| id|age|height|weight|ap_hi|ap_lo|cholesterol|glucose|smoke|alcohol|active|cardio|gender|\n",
      "+---+---+------+------+-----+-----+-----------+-------+-----+-------+------+------+------+\n",
      "|  0|  1|     0|     1|    4|    4|          4|      4|    7|      8|     3|     3|     0|\n",
      "+---+---+------+------+-----+-----+-----------+-------+-----+-------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, when, col\n",
    "\n",
    "def count_outliers(df, column):\n",
    "    # Compute Q1, Q3, and IQR\n",
    "    Q1, Q3 = df.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Filter rows that are considered outliers\n",
    "    outliers = df.filter((pyspark.sql.functions.col(column) < (Q1 - 1.5 * IQR)) | (pyspark.sql.functions.col(column) > (Q3 + 1.5 * IQR)))\n",
    "    \n",
    "    return outliers.count()\n",
    "\n",
    "def count_extremes(df, column):\n",
    "    # Compute Q1, Q3, and IQR\n",
    "    Q1, Q3 = df.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Filter rows that are considered extremes\n",
    "    extremes = df.filter((pyspark.sql.functions.col(column) < (Q1 - 3 * IQR)) | (pyspark.sql.functions.col(column) > (Q3 + 3 * IQR)))\n",
    "    \n",
    "    return extremes.count()\n",
    "\n",
    "# Report on Outliers and Extremes in the selected data\n",
    "print(\"Before cleaning:\")\n",
    "for col in selected_columns:\n",
    "    print(f\"{col} has {count_outliers(data_selected, col)} outliers.\")\n",
    "    print(f\"{col} has {count_extremes(data_selected, col)} extreme values.\")\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data_selected.select([count(when(pyspark.sql.functions.col(c).isNull(), c)).alias(c) for c in selected_columns])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "327c1bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1st cleaning:\n",
      "id has 0 outliers.\n",
      "id has 0 extreme values.\n",
      "age has 4 outliers.\n",
      "age has 0 extreme values.\n",
      "gender has 0 outliers.\n",
      "gender has 0 extreme values.\n",
      "height has 367 outliers.\n",
      "height has 0 extreme values.\n",
      "weight has 1688 outliers.\n",
      "weight has 150 extreme values.\n",
      "ap_hi has 1229 outliers.\n",
      "ap_hi has 221 extreme values.\n",
      "ap_lo has 3600 outliers.\n",
      "ap_lo has 130 extreme values.\n",
      "cholesterol has 17193 outliers.\n",
      "cholesterol has 17193 extreme values.\n",
      "glucose has 10302 outliers.\n",
      "glucose has 10302 extreme values.\n",
      "smoke has 6047 outliers.\n",
      "smoke has 6047 extreme values.\n",
      "alcohol has 3684 outliers.\n",
      "alcohol has 3684 extreme values.\n",
      "active has 13516 outliers.\n",
      "active has 13516 extreme values.\n",
      "cardio has 0 outliers.\n",
      "cardio has 0 extreme values.\n",
      "\n",
      "---\n",
      "\n",
      "+---+---+------+------+------+-----+-----+-----------+-------+-----+-------+------+------+\n",
      "| id|age|gender|height|weight|ap_hi|ap_lo|cholesterol|glucose|smoke|alcohol|active|cardio|\n",
      "+---+---+------+------+------+-----+-----+-----------+-------+-----+-------+------+------+\n",
      "|  0|  0|     0|     0|     0|    0|    0|          0|      0|    0|      0|     0|     0|\n",
      "+---+---+------+------+------+-----+-----+-----------+-------+-----+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Removing unrealistic or erroneous data for age, height, weight, and blood pressures\n",
    "data_clean = df.filter(\n",
    "    (col('age') >= 18) & (col('age') <= 110) &\n",
    "    (col('height') >= 130) & (col('height') <= 220) &\n",
    "    (col('weight') >= 40) & (col('weight') <= 180) &\n",
    "    (col('ap_hi') > 0) & (col('ap_hi') < 250) &\n",
    "    (col('ap_lo') > 0) & (col('ap_lo') < 150)\n",
    ")\n",
    "\n",
    "# Ensuring binary columns contain only 0 or 1\n",
    "binary_cols = ['smoke', 'alcohol', 'active', 'cardio']\n",
    "for b_col in binary_cols:\n",
    "    data_clean = data_clean.filter(col(b_col).isin([0, 1]))\n",
    "\n",
    "# Ensuring 'cholesterol' and 'glucose' only contain values 1, 2, and 3, and removing other rows\n",
    "category_cols = ['cholesterol', 'glucose']\n",
    "for c_col in category_cols:\n",
    "    data_clean = data_clean.filter(col(c_col).isin([1, 2, 3]))\n",
    "\n",
    "# Replace outliers with median for height\n",
    "height_median = data_clean.approxQuantile('height', [0.5], 0.01)[0]\n",
    "Q1, Q3 = data_clean.approxQuantile('height', [0.25, 0.75], 0.01)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "data_clean = data_clean.withColumn(\n",
    "    'height',\n",
    "    when((col('height') < (Q1 - 1.5 * IQR)) | (col('height') > (Q3 + 1.5 * IQR)), height_median).otherwise(col('height'))\n",
    ")\n",
    "\n",
    "# Outlier and extreme values reporting\n",
    "print(\"After 1st cleaning:\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col} has {count_outliers(data_clean, col)} outliers.\")\n",
    "    print(f\"{col} has {count_extremes(data_clean, col)} extreme values.\")\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "# For missing values\n",
    "missing_values_after_cleaning = data_clean.select([count(when(pyspark.sql.functions.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "missing_values_after_cleaning.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b672ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After handling outliers, 2nd cleaning:\n",
      "id has 0 outliers.\n",
      "id has 0 extreme values.\n",
      "age has 0 outliers.\n",
      "age has 0 extreme values.\n",
      "gender has 0 outliers.\n",
      "gender has 0 extreme values.\n",
      "height has 367 outliers.\n",
      "height has 0 extreme values.\n",
      "weight has 1141 outliers.\n",
      "weight has 0 extreme values.\n",
      "ap_hi has 0 outliers.\n",
      "ap_hi has 0 extreme values.\n",
      "ap_lo has 0 outliers.\n",
      "ap_lo has 0 extreme values.\n",
      "cholesterol has 17193 outliers.\n",
      "cholesterol has 17193 extreme values.\n",
      "glucose has 10302 outliers.\n",
      "glucose has 10302 extreme values.\n",
      "smoke has 6047 outliers.\n",
      "smoke has 6047 extreme values.\n",
      "alcohol has 3684 outliers.\n",
      "alcohol has 3684 extreme values.\n",
      "active has 13516 outliers.\n",
      "active has 13516 extreme values.\n",
      "cardio has 0 outliers.\n",
      "cardio has 0 extreme values.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "def replace_outliers_with_median(df, column):\n",
    "    # Compute Q1, Q3, IQR, and median\n",
    "    Q1, Q3 = df.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "    IQR = Q3 - Q1\n",
    "    median_val = df.approxQuantile(column, [0.5], 0.01)[0]\n",
    "\n",
    "    # Replace values that are considered outliers with the median\n",
    "    return df.withColumn(column, when((pyspark.sql.functions.col(column) < (Q1 - 1.5 * IQR)) | (pyspark.sql.functions.col(column) > (Q3 + 1.5 * IQR)), median_val).otherwise(pyspark.sql.functions.col(column)))\n",
    "\n",
    "# Apply the function\n",
    "for col in ['weight', 'ap_hi', 'ap_lo', 'age']:\n",
    "    data_clean = replace_outliers_with_median(data_clean, col)\n",
    "\n",
    "# Checking again for outliers and extremes\n",
    "print(\"After handling outliers, 2nd cleaning:\")\n",
    "for col in data_clean.columns:\n",
    "    print(f\"{col} has {count_outliers(data_clean, col)} outliers.\")\n",
    "    print(f\"{col} has {count_extremes(data_clean, col)} extreme values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c8d35d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After handling outliers, 3rd cleaning:\n",
      "id has 0 outliers.\n",
      "id has 0 extreme values.\n",
      "age has 0 outliers.\n",
      "age has 0 extreme values.\n",
      "gender has 0 outliers.\n",
      "gender has 0 extreme values.\n",
      "height has 0 outliers.\n",
      "height has 0 extreme values.\n",
      "weight has 1141 outliers.\n",
      "weight has 0 extreme values.\n",
      "ap_hi has 0 outliers.\n",
      "ap_hi has 0 extreme values.\n",
      "ap_lo has 0 outliers.\n",
      "ap_lo has 0 extreme values.\n",
      "cholesterol has 17193 outliers.\n",
      "cholesterol has 17193 extreme values.\n",
      "glucose has 10302 outliers.\n",
      "glucose has 10302 extreme values.\n",
      "smoke has 6047 outliers.\n",
      "smoke has 6047 extreme values.\n",
      "alcohol has 3684 outliers.\n",
      "alcohol has 3684 extreme values.\n",
      "active has 13516 outliers.\n",
      "active has 13516 extreme values.\n",
      "cardio has 0 outliers.\n",
      "cardio has 0 extreme values.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "# For the height outlier handling, calculate IQR first.\n",
    "Q1_height, Q3_height = data_clean.approxQuantile(\"height\", [0.25, 0.75], 0.01)\n",
    "IQR_height = Q3_height - Q1_height\n",
    "median_height = data_clean.approxQuantile(\"height\", [0.5], 0.01)[0]\n",
    "\n",
    "# Replace height outliers with median\n",
    "data_clean = data_clean.withColumn(\"height\", when((pyspark.sql.functions.col(\"height\") < (Q1_height - 1.5 * IQR_height)) | \n",
    "                                                  (pyspark.sql.functions.col(\"height\") > (Q3_height + 1.5 * IQR_height)), \n",
    "                                                  median_height).otherwise(pyspark.sql.functions.col(\"height\")))\n",
    "\n",
    "# Handle categorical and binary fields with outliers\n",
    "categorical_dict = {\n",
    "    \"cholesterol\": [1, 2, 3],\n",
    "    \"glucose\": [1, 2, 3],\n",
    "    \"smoke\": [0, 1],\n",
    "    \"alcohol\": [0, 1],\n",
    "    \"active\": [0, 1]\n",
    "}\n",
    "\n",
    "for column, valid_values in categorical_dict.items():\n",
    "    mode_val = data_clean.groupBy(column).count().orderBy(\"count\", ascending=False).limit(1).select(column).collect()[0][0]\n",
    "    data_clean = data_clean.withColumn(column, when(pyspark.sql.functions.col(column).isin(valid_values), pyspark.sql.functions.col(column)).otherwise(lit(mode_val)))\n",
    "\n",
    "# Checking again for outliers and extremes after the third cleaning\n",
    "print(\"After handling outliers, 3rd cleaning:\")\n",
    "for column in data_clean.columns:\n",
    "    print(f\"{column} has {count_outliers(data_clean, column)} outliers.\")\n",
    "    print(f\"{column} has {count_extremes(data_clean, column)} extreme values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c227a56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows Left After Data Cleaning: 68742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: id\n",
      "Max value: 69999\n",
      "Min value: 1\n",
      "----------------------------------------\n",
      "Column: age\n",
      "Max value: 65.0\n",
      "Min value: 39.0\n",
      "----------------------------------------\n",
      "Column: gender\n",
      "Max value: 1\n",
      "Min value: 0\n",
      "----------------------------------------\n",
      "Column: height\n",
      "Max value: 184.0\n",
      "Min value: 144.0\n",
      "----------------------------------------\n",
      "Column: weight\n",
      "Max value: 107.0\n",
      "Min value: 40.0\n",
      "----------------------------------------\n",
      "Column: ap_hi\n",
      "Max value: 170.0\n",
      "Min value: 90.0\n",
      "----------------------------------------\n",
      "Column: ap_lo\n",
      "Max value: 105.0\n",
      "Min value: 65.0\n",
      "----------------------------------------\n",
      "Column: cholesterol\n",
      "Max value: 3\n",
      "Min value: 1\n",
      "----------------------------------------\n",
      "Column: glucose\n",
      "Max value: 3\n",
      "Min value: 1\n",
      "----------------------------------------\n",
      "Column: smoke\n",
      "Max value: 1\n",
      "Min value: 0\n",
      "----------------------------------------\n",
      "Column: alcohol\n",
      "Max value: 1\n",
      "Min value: 0\n",
      "----------------------------------------\n",
      "Column: active\n",
      "Max value: 1\n",
      "Min value: 0\n",
      "----------------------------------------\n",
      "Column: cardio\n",
      "Max value: 1\n",
      "Min value: 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min\n",
    "\n",
    "# Count the number of rows\n",
    "print(f\"Number of Rows Left After Data Cleaning: {data_clean.count()}\")\n",
    "\n",
    "# Save the cleaned DataFrame to CSV\n",
    "data_clean.write.csv(\"output_data.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# Print max and min values for each column\n",
    "for column in data_clean.columns:\n",
    "    max_val = data_clean.agg(max(column)).collect()[0][0]\n",
    "    min_val = data_clean.agg(min(column)).collect()[0][0]\n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"Max value: {max_val}\")\n",
    "    print(f\"Min value: {min_val}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b51a07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 68742\n",
      "+---+----+------+------+------+-----+-----+-----------+-------+-----+-------+------+------+------+\n",
      "| id| age|gender|height|weight|ap_hi|ap_lo|cholesterol|glucose|smoke|alcohol|active|cardio|   BMI|\n",
      "+---+----+------+------+------+-----+-----+-----------+-------+-----+-------+------+------+------+\n",
      "|  1|55.0|     1| 156.0|  85.0|140.0| 90.0|          3|      1|    0|      0|     1|     1|34.928|\n",
      "|  2|52.0|     1| 165.0|  64.0|130.0| 70.0|          3|      1|    0|      0|     0|     1|23.508|\n",
      "|  3|48.0|     0| 169.0|  82.0|150.0|100.0|          1|      1|    0|      0|     1|     1| 28.71|\n",
      "|  4|48.0|     1| 156.0|  56.0|100.0| 80.0|          1|      1|    0|      0|     0|     0|23.011|\n",
      "|  5|60.0|     1| 151.0|  67.0|120.0| 80.0|          2|      2|    0|      0|     0|     0|29.385|\n",
      "|  7|62.0|     0| 178.0|  95.0|130.0| 90.0|          3|      3|    0|      0|     1|     1|29.984|\n",
      "|  8|48.0|     1| 158.0|  71.0|110.0| 70.0|          1|      1|    0|      0|     1|     0|28.441|\n",
      "|  9|54.0|     1| 164.0|  68.0|110.0| 80.0|          1|      1|    0|      0|     0|     0|25.283|\n",
      "| 11|52.0|     0| 173.0|  60.0|120.0| 80.0|          1|      1|    0|      0|     1|     0|20.047|\n",
      "| 13|54.0|     1| 158.0|  78.0|110.0| 70.0|          1|      1|    0|      0|     1|     0|31.245|\n",
      "+---+----+------+------+------+-----+-----+-----------+-------+-----+-------+------+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 618:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+-------+-------+-------+-------+-------+-----------+-------+-------+-------+-------+-------+\n",
      "|summary|      id|    age| gender| height| weight|  ap_hi|  ap_lo|cholesterol|glucose|  smoke|alcohol| active| cardio|\n",
      "+-------+--------+-------+-------+-------+-------+-------+-------+-----------+-------+-------+-------+-------+-------+\n",
      "|  count| 68742.0|68742.0|68742.0|68742.0|68742.0|68742.0|68742.0|    68742.0|68742.0|68742.0|68742.0|68742.0|68742.0|\n",
      "|   mean|35028.01|  53.33|   0.65| 164.33|  72.99| 125.81|  81.83|       1.36|   1.23|   0.09|   0.05|    0.8|   0.49|\n",
      "| stddev|20185.89|   6.77|   0.48|   7.42|  12.26|  15.08|   7.63|       0.68|   0.57|   0.28|   0.23|    0.4|    0.5|\n",
      "|    min|     1.0|   39.0|    0.0|  144.0|   40.0|   90.0|   65.0|        1.0|    1.0|    0.0|    0.0|    0.0|    0.0|\n",
      "|    max| 69999.0|   65.0|    1.0|  184.0|  107.0|  170.0|  105.0|        3.0|    3.0|    1.0|    1.0|    1.0|    1.0|\n",
      "+-------+--------+-------+-------+-------+-------+-------+-------+-----------+-------+-------+-------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Create a copy of df1 as dfc\n",
    "dfc = data_clean\n",
    "\n",
    "# Print the count of rows\n",
    "print(f\"Number of Rows: {dfc.count()}\")\n",
    "\n",
    "# Create a new column BMI in the dfc DataFrame\n",
    "dfc = dfc.withColumn(\"BMI\", round(dfc[\"weight\"] / (dfc[\"height\"] / 100)**2, 3))\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "dfc.show(10)\n",
    "\n",
    "# Save the dfc DataFrame to a CSV file\n",
    "dfc.write.csv(\"output_data3.3.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "from pyspark.sql.functions import round, col\n",
    "\n",
    "# Rounding values to 2 decimal places (for select columns)\n",
    "rounded_desc = dfc.select(df.columns).describe()\n",
    "for col_name in rounded_desc.columns[1:]:  # skip the summary column\n",
    "    rounded_desc = rounded_desc.withColumn(col_name, round(col(col_name), 2))\n",
    "rounded_desc.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c083afc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- ap_hi: double (nullable = true)\n",
      " |-- ap_lo: double (nullable = true)\n",
      " |-- cholesterol: integer (nullable = true)\n",
      " |-- glucose: integer (nullable = true)\n",
      " |-- smoke: integer (nullable = true)\n",
      " |-- alcohol: integer (nullable = true)\n",
      " |-- active: integer (nullable = true)\n",
      " |-- cardio: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      "\n",
      "+---+----+------+------+-----+-----+-----------+-------+-----+-------+------+------+------+------+\n",
      "| id| age|height|weight|ap_hi|ap_lo|cholesterol|glucose|smoke|alcohol|active|cardio|   BMI|gender|\n",
      "+---+----+------+------+-----+-----+-----------+-------+-----+-------+------+------+------+------+\n",
      "|  1|55.0| 156.0|  85.0|140.0| 90.0|          3|      1|    0|      0|     1|     1|34.928|     0|\n",
      "|  2|52.0| 165.0|  64.0|130.0| 70.0|          3|      1|    0|      0|     0|     1|23.508|     0|\n",
      "|  3|48.0| 169.0|  82.0|150.0|100.0|          1|      1|    0|      0|     1|     1| 28.71|     1|\n",
      "|  4|48.0| 156.0|  56.0|100.0| 80.0|          1|      1|    0|      0|     0|     0|23.011|     0|\n",
      "|  5|60.0| 151.0|  67.0|120.0| 80.0|          2|      2|    0|      0|     0|     0|29.385|     0|\n",
      "|  7|62.0| 178.0|  95.0|130.0| 90.0|          3|      3|    0|      0|     1|     1|29.984|     1|\n",
      "|  8|48.0| 158.0|  71.0|110.0| 70.0|          1|      1|    0|      0|     1|     0|28.441|     0|\n",
      "|  9|54.0| 164.0|  68.0|110.0| 80.0|          1|      1|    0|      0|     0|     0|25.283|     0|\n",
      "| 11|52.0| 173.0|  60.0|120.0| 80.0|          1|      1|    0|      0|     1|     0|20.047|     1|\n",
      "| 13|54.0| 158.0|  78.0|110.0| 70.0|          1|      1|    0|      0|     1|     0|31.245|     0|\n",
      "+---+----+------+------+-----+-----+-----------+-------+-----+-------+------+------+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('merge_data').getOrCreate()\n",
    "\n",
    "# 1. Drop the 'gender' column from dfc\n",
    "dmerge = dfc.drop('gender')\n",
    "\n",
    "# 2. Save the cleaned data (skip if you don't want to save to intermediate file)\n",
    "dmerge.write.csv(\"output_data3.4_spark.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# 3. Read the saved cleaned data and the external dataset\n",
    "d1 = spark.read.csv(\"output_data3.4_spark.csv\", header=True, inferSchema=True)\n",
    "d2 = spark.read.csv(\"HeartData3.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 4. Ensure the 'id' columns in both datasets have the same datatype\n",
    "d1 = d1.withColumn(\"id\", d1[\"id\"].cast(IntegerType()))\n",
    "d2 = d2.withColumn(\"id\", d2[\"id\"].cast(IntegerType()))\n",
    "\n",
    "# 5. Merge the dataframes based on a common column 'id'\n",
    "merged_df = d1.join(d2, on=\"id\", how=\"inner\")\n",
    "\n",
    "# 6. Save the merged dataframe to a new CSV\n",
    "merged_df.write.csv(\"merged_file_spark.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# 7. Print the info of the merged dataframe\n",
    "merged_df.printSchema()\n",
    "\n",
    "merged_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76c5d7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully converted and saved.\n",
      "{\"id\":1,\"age\":55.0,\"gender\":1,\"height\":156.0,\"weight\":85.0,\"ap_hi\":140.0,\"ap_lo\":90.0,\"cholesterol\":3,\"glucose\":1,\"smoke\":0,\"alcohol\":0,\"active\":1,\"cardio\":1,\"BMI\":34.928}\n",
      "\n",
      "{\"id\":2,\"age\":52.0,\"gender\":1,\"height\":165.0,\"weight\":64.0,\"ap_hi\":130.0,\"ap_lo\":70.0,\"cholesterol\":3,\"glucose\":1,\"smoke\":0,\"alcohol\":0,\"active\":0,\"cardio\":1,\"BMI\":23.508}\n",
      "\n",
      "{\"id\":3,\"age\":48.0,\"gender\":0,\"height\":169.0,\"weight\":82.0,\"ap_hi\":150.0,\"ap_lo\":100.0,\"cholesterol\":1,\"glucose\":1,\"smoke\":0,\"alcohol\":0,\"active\":1,\"cardio\":1,\"BMI\":28.71}\n",
      "\n",
      "{\"id\":4,\"age\":48.0,\"gender\":1,\"height\":156.0,\"weight\":56.0,\"ap_hi\":100.0,\"ap_lo\":80.0,\"cholesterol\":1,\"glucose\":1,\"smoke\":0,\"alcohol\":0,\"active\":0,\"cardio\":0,\"BMI\":23.011}\n",
      "\n",
      "{\"id\":5,\"age\":60.0,\"gender\":1,\"height\":151.0,\"weight\":67.0,\"ap_hi\":120.0,\"ap_lo\":80.0,\"cholesterol\":2,\"glucose\":2,\"smoke\":0,\"alcohol\":0,\"active\":0,\"cardio\":0,\"BMI\":29.385}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('csv_to_json').getOrCreate()\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = spark.read.csv('output_data3.3.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Step 2: Save the dataframe to a JSON directory\n",
    "output_path = 'output_data3.3.json'\n",
    "df.write.mode('overwrite').json(output_path)  # Note the mode('overwrite')\n",
    "\n",
    "print(\"Data has been successfully converted and saved.\")\n",
    "\n",
    "# List all the files inside the directory\n",
    "json_files = os.listdir(output_path)\n",
    "\n",
    "# To display the first few lines of the first JSON file (for verification purposes)\n",
    "with open(os.path.join(output_path, json_files[0]), 'r') as file:\n",
    "    for _ in range(5):  # Let's display first 5 lines as an example\n",
    "        print(file.readline())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3c74d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+------+------+-----+-----+-----+-------+------+------+------+-----------+-------+\n",
      "| id| age|gender|height|weight|ap_hi|ap_lo|smoke|alcohol|active|cardio|   BMI|cholesterol|glucose|\n",
      "+---+----+------+------+------+-----+-----+-----+-------+------+------+------+-----------+-------+\n",
      "|  1|55.0|     1| 156.0|  85.0|140.0| 90.0|    0|      0|     1|     1|34.928|        2.0|    0.0|\n",
      "|  2|52.0|     1| 165.0|  64.0|130.0| 70.0|    0|      0|     0|     1|23.508|        2.0|    0.0|\n",
      "|  3|48.0|     0| 169.0|  82.0|150.0|100.0|    0|      0|     1|     1| 28.71|        0.0|    0.0|\n",
      "|  4|48.0|     1| 156.0|  56.0|100.0| 80.0|    0|      0|     0|     0|23.011|        0.0|    0.0|\n",
      "|  5|60.0|     1| 151.0|  67.0|120.0| 80.0|    0|      0|     0|     0|29.385|        1.0|    2.0|\n",
      "|  7|62.0|     0| 178.0|  95.0|130.0| 90.0|    0|      0|     1|     1|29.984|        2.0|    1.0|\n",
      "|  8|48.0|     1| 158.0|  71.0|110.0| 70.0|    0|      0|     1|     0|28.441|        0.0|    0.0|\n",
      "|  9|54.0|     1| 164.0|  68.0|110.0| 80.0|    0|      0|     0|     0|25.283|        0.0|    0.0|\n",
      "| 11|52.0|     0| 173.0|  60.0|120.0| 80.0|    0|      0|     1|     0|20.047|        0.0|    0.0|\n",
      "| 13|54.0|     1| 158.0|  78.0|110.0| 70.0|    0|      0|     1|     0|31.245|        0.0|    0.0|\n",
      "+---+----+------+------+------+-----+-----+-----+-------+------+------+------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- ap_hi: double (nullable = true)\n",
      " |-- ap_lo: double (nullable = true)\n",
      " |-- smoke: integer (nullable = true)\n",
      " |-- alcohol: integer (nullable = true)\n",
      " |-- active: integer (nullable = true)\n",
      " |-- cardio: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- cholesterol: double (nullable = false)\n",
      " |-- glucose: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('category_conversion').getOrCreate()\n",
    "\n",
    "# Use StringIndexer to convert the columns to category type\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(dfc) for column in [\"cholesterol\", \"glucose\"]]\n",
    "\n",
    "# Apply the transformation\n",
    "for indexer in indexers:\n",
    "    dfc = indexer.transform(dfc).drop(indexer.getInputCol())\n",
    "\n",
    "# Rename the new columns back to original names\n",
    "dfc = dfc.withColumnRenamed(\"cholesterol_index\", \"cholesterol\").withColumnRenamed(\"glucose_index\", \"glucose\")\n",
    "\n",
    "# Show transformed dataframe\n",
    "dfc.show(10)\n",
    "\n",
    "\n",
    "# Assuming dfc is your DataFrame\n",
    "dfc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf7e9b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- ap_hi: double (nullable = true)\n",
      " |-- ap_lo: double (nullable = true)\n",
      " |-- smoke: integer (nullable = true)\n",
      " |-- alcohol: integer (nullable = true)\n",
      " |-- active: integer (nullable = true)\n",
      " |-- cardio: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- cholesterol: integer (nullable = true)\n",
      " |-- glucose: integer (nullable = true)\n",
      "\n",
      "+---+----+------+------+------+-----+-----+-----+-------+------+------+------+-----------+-------+\n",
      "| id| age|gender|height|weight|ap_hi|ap_lo|smoke|alcohol|active|cardio|   BMI|cholesterol|glucose|\n",
      "+---+----+------+------+------+-----+-----+-----+-------+------+------+------+-----------+-------+\n",
      "|  1|55.0|     1| 156.0|  85.0|140.0| 90.0|    0|      0|     1|     1|34.928|          2|      0|\n",
      "|  2|52.0|     1| 165.0|  64.0|130.0| 70.0|    0|      0|     0|     1|23.508|          2|      0|\n",
      "|  3|48.0|     0| 169.0|  82.0|150.0|100.0|    0|      0|     1|     1| 28.71|          0|      0|\n",
      "|  4|48.0|     1| 156.0|  56.0|100.0| 80.0|    0|      0|     0|     0|23.011|          0|      0|\n",
      "|  5|60.0|     1| 151.0|  67.0|120.0| 80.0|    0|      0|     0|     0|29.385|          1|      2|\n",
      "|  7|62.0|     0| 178.0|  95.0|130.0| 90.0|    0|      0|     1|     1|29.984|          2|      1|\n",
      "|  8|48.0|     1| 158.0|  71.0|110.0| 70.0|    0|      0|     1|     0|28.441|          0|      0|\n",
      "|  9|54.0|     1| 164.0|  68.0|110.0| 80.0|    0|      0|     0|     0|25.283|          0|      0|\n",
      "| 11|52.0|     0| 173.0|  60.0|120.0| 80.0|    0|      0|     1|     0|20.047|          0|      0|\n",
      "| 13|54.0|     1| 158.0|  78.0|110.0| 70.0|    0|      0|     1|     0|31.245|          0|      0|\n",
      "+---+----+------+------+------+-----+-----+-----+-------+------+------+------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert 'cholesterol' and 'glucose' to string\n",
    "dfc = dfc.withColumn(\"cholesterol\", col(\"cholesterol\").cast(\"string\"))\n",
    "dfc = dfc.withColumn(\"glucose\", col(\"glucose\").cast(\"string\"))\n",
    "\n",
    "# Show the schema and the data\n",
    "#dfc.printSchema()\n",
    "#dfc.show(10)\n",
    "\n",
    "# Convert 'cholesterol' and 'glucose' to string\n",
    "dfc = dfc.withColumn(\"cholesterol\", col(\"cholesterol\").cast(\"integer\"))\n",
    "dfc = dfc.withColumn(\"glucose\", col(\"glucose\").cast(\"integer\"))\n",
    "\n",
    "# Show the schema and the data\n",
    "dfc.printSchema()\n",
    "dfc.show(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
